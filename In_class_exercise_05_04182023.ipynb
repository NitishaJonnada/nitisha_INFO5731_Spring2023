{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitishaJonnada/nitisha_INFO5731_Spring2023/blob/main/In_class_exercise_05_04182023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGiy3gTTi59j"
      },
      "source": [
        "# **The fifth in-class-exercise (40 points in total, 4/18/2023)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3Tc_Yo_i59l"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text classification as well as the performance evaluation. In addition, you are requried to conduct *10 fold cross validation (https://scikit-learn.org/stable/modules/cross_validation.html)* in the training. \n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data. \n",
        "\n",
        "Algorithms:\n",
        "\n",
        "(1) MultinominalNB\n",
        "\n",
        "(2) SVM \n",
        "\n",
        "(3) KNN \n",
        "\n",
        "(4) Decision tree\n",
        "\n",
        "(5) Random Forest\n",
        "\n",
        "(6) XGBoost\n",
        "\n",
        "(7) Word2Vec\n",
        "\n",
        "(8) BERT\n",
        "\n",
        "Evaluation measurement:\n",
        "\n",
        "(1) Accuracy\n",
        "\n",
        "(2) Recall\n",
        "\n",
        "(3) Precison \n",
        "\n",
        "(4) F-1 score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "14HQB_j_i8j7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPmYe9Z6i59l",
        "outputId": "52da910b-c773-4976-a4ad-9000d5f1e0fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, svm\n",
        "from sklearn import naive_bayes\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "nltk.download('wordnet')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "RHS0FONpjYfV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reading_data(file_path):\n",
        "  text_data, sentiments = [], []\n",
        "  file_data = open(file_path).read()\n",
        "  for i, j in enumerate(file_data.split(\"\\n\")):\n",
        "    after_split = j.split(' ')\n",
        "    text_data.append(\" \".join(after_split[1:]))\n",
        "    sentiments.append(after_split[0])\n",
        "  return text_data, sentiments"
      ],
      "metadata": {
        "id": "A4CtwtwGjanO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_text_data, training_sentiments = reading_data('stsa-train.txt')\n",
        "training_df = pd.DataFrame(list(zip(training_sentiments, training_text_data)), columns = ['Sentimental Value', 'Raw Data'])\n",
        "testing_text_data, testing_sentiments = reading_data('stsa-test.txt')\n",
        "testing_df = pd.DataFrame(list(zip(testing_sentiments, testing_text_data)), columns = ['Sentimental Value', 'Raw Data'])"
      ],
      "metadata": {
        "id": "KwAkU7hmjfz0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pre-processing\n",
        "#removal of special characters\n",
        "training_df['After noise removal'] = training_df['Raw Data'].apply(lambda x: ''.join(re.sub(r\"[^a-zA-Z0-9]+\", ' ', charctr) for charctr in x ))\n",
        "testing_df['After noise removal'] = testing_df['Raw Data'].apply(lambda x: ''.join(re.sub(r\"[^a-zA-Z0-9]+\", ' ', charctr) for charctr in x ))\n",
        "# removal of Punctuation\n",
        "training_df['Punctuation removal'] = training_df['After noise removal'].str.replace('[^\\w\\s]','')\n",
        "testing_df['Punctuation removal'] = testing_df['After noise removal'].str.replace('[^\\w\\s]','')\n",
        "# Stopwords removal\n",
        "stop_word = stopwords.words('english')\n",
        "training_df['Stopwords removal'] = training_df['Punctuation removal'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_word))\n",
        "testing_df['Stopwords removal'] = testing_df['Punctuation removal'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_word))\n",
        "# Lower Casing\n",
        "training_df['Lower casing'] = training_df['Stopwords removal'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "testing_df['Lower casing'] = testing_df['Stopwords removal'].apply(lambda x: \" \".join(x.lower() for x in x.split()))"
      ],
      "metadata": {
        "id": "HpB1LSKmkiyB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vector = TfidfVectorizer(analyzer = 'word')\n",
        "tfidf_vector.fit(training_df['Lower casing'])\n",
        "x =  tfidf_vector.transform(training_df['Lower casing'])\n",
        "tfidf_vector_test = TfidfVectorizer(analyzer='word', vocabulary = tfidf_vector.vocabulary_)\n",
        "tfidf_vector_test.fit(testing_df['Lower casing'])\n",
        "test_values_x = tfidf_vector_test.transform(testing_df['Lower casing'])\n"
      ],
      "metadata": {
        "id": "vsfFYmKgk5iV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain, xvalid, ytrain, yvalid = model_selection.train_test_split(x, training_df['Sentimental Value'],test_size=0.2)"
      ],
      "metadata": {
        "id": "EFFGkaWSlykg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def csv(model, x_data, y_data):\n",
        "  scoring = 'accuracy'\n",
        "  kfold = KFold(10, random_state = 7, shuffle=True)\n",
        "  return cross_val_score(model, x_data, y_data, cv=kfold).mean()"
      ],
      "metadata": {
        "id": "_lDcnprlmJuY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_model(model_intializer):\n",
        "  model = model_intializer\n",
        "  model.fit(xtrain, ytrain)\n",
        "  predicted = model.predict(xvalid)\n",
        "  accuracy = accuracy_score(predicted, yvalid)\n",
        "  print(\"Accuracy of Traning data: {0}\".format(accuracy))\n",
        "  print(classification_report(yvalid, predicted))\n",
        "  predicted_testing = model.predict(test_values_x)\n",
        "  accuracy_testing = accuracy_score(predicted_testing, testing_df['Sentimental Value'])\n",
        "  print(\"Accuracy of Testing data: {0}\".format(accuracy_testing))\n",
        "  print(classification_report(testing_df['Sentimental Value'], predicted_testing))\n",
        "  if 'XGB' not in str(model):\n",
        "    print(\"Cross validation score obtained: {0}\".format(csv(model, test_values_x, testing_df['Sentimental Value'])))"
      ],
      "metadata": {
        "id": "BqMJBYjkmOCT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_model(naive_bayes.MultinomialNB())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PHtwm4tmaBh",
        "outputId": "d0842556-673d-4b4d-edb0-50e15936217e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Traning data: 0.8007220216606499\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.73      0.78       671\n",
            "           1       0.77      0.87      0.82       714\n",
            "\n",
            "    accuracy                           0.80      1385\n",
            "   macro avg       0.81      0.80      0.80      1385\n",
            "weighted avg       0.80      0.80      0.80      1385\n",
            "\n",
            "Accuracy of Testing data: 0.7903402854006586\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "                   0.00      0.00      0.00         1\n",
            "           0       0.86      0.70      0.77       912\n",
            "           1       0.74      0.88      0.81       909\n",
            "\n",
            "    accuracy                           0.79      1822\n",
            "   macro avg       0.53      0.53      0.53      1822\n",
            "weighted avg       0.80      0.79      0.79      1822\n",
            "\n",
            "Cross validation score obtained: 0.7365309553834145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_model(svm.SVC())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UibIzc-Tmcwk",
        "outputId": "54db7335-e2bd-43b4-f501-c180bba7cd65"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Traning data: 0.7935018050541516\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.76      0.78       671\n",
            "           1       0.78      0.83      0.80       714\n",
            "\n",
            "    accuracy                           0.79      1385\n",
            "   macro avg       0.79      0.79      0.79      1385\n",
            "weighted avg       0.79      0.79      0.79      1385\n",
            "\n",
            "Accuracy of Testing data: 0.7925356750823271\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "                   0.00      0.00      0.00         1\n",
            "           0       0.82      0.75      0.78       912\n",
            "           1       0.77      0.83      0.80       909\n",
            "\n",
            "    accuracy                           0.79      1822\n",
            "   macro avg       0.53      0.53      0.53      1822\n",
            "weighted avg       0.79      0.79      0.79      1822\n",
            "\n",
            "Cross validation score obtained: 0.7217017954722873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_model(KNeighborsClassifier(n_neighbors = 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omWjLlHLmq-F",
        "outputId": "41604ed7-8b0b-4c97-bd48-24122047b70d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Traning data: 0.5292418772563177\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.04      0.07       671\n",
            "           1       0.52      0.99      0.68       714\n",
            "\n",
            "    accuracy                           0.53      1385\n",
            "   macro avg       0.66      0.51      0.38      1385\n",
            "weighted avg       0.66      0.53      0.39      1385\n",
            "\n",
            "Accuracy of Testing data: 0.5104281009879253\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "                   0.00      0.00      0.00         1\n",
            "           0       0.71      0.04      0.07       912\n",
            "           1       0.50      0.98      0.67       909\n",
            "\n",
            "    accuracy                           0.51      1822\n",
            "   macro avg       0.41      0.34      0.25      1822\n",
            "weighted avg       0.61      0.51      0.37      1822\n",
            "\n",
            "Cross validation score obtained: 0.5005464480874318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_model(DecisionTreeClassifier())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scaaaxH4mvJA",
        "outputId": "c7cc5e65-ebaf-43aa-e953-7a6489fa759a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Traning data: 0.6765342960288808\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.71      0.68       671\n",
            "           1       0.70      0.64      0.67       714\n",
            "\n",
            "    accuracy                           0.68      1385\n",
            "   macro avg       0.68      0.68      0.68      1385\n",
            "weighted avg       0.68      0.68      0.68      1385\n",
            "\n",
            "Accuracy of Testing data: 0.6580680570801317\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "                   0.00      0.00      0.00         1\n",
            "           0       0.65      0.67      0.66       912\n",
            "           1       0.66      0.64      0.65       909\n",
            "\n",
            "    accuracy                           0.66      1822\n",
            "   macro avg       0.44      0.44      0.44      1822\n",
            "weighted avg       0.66      0.66      0.66      1822\n",
            "\n",
            "Cross validation score obtained: 0.5977121239416322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_model(RandomForestClassifier())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjbWDpbim2PQ",
        "outputId": "d817d9b3-1ce3-458e-e65f-47eb2622037c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Traning data: 0.7415162454873646\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.76      0.74       671\n",
            "           1       0.76      0.72      0.74       714\n",
            "\n",
            "    accuracy                           0.74      1385\n",
            "   macro avg       0.74      0.74      0.74      1385\n",
            "weighted avg       0.74      0.74      0.74      1385\n",
            "\n",
            "Accuracy of Testing data: 0.7447859495060373\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "                   0.00      0.00      0.00         1\n",
            "           0       0.74      0.76      0.75       912\n",
            "           1       0.75      0.73      0.74       909\n",
            "\n",
            "    accuracy                           0.74      1822\n",
            "   macro avg       0.50      0.50      0.50      1822\n",
            "weighted avg       0.74      0.74      0.74      1822\n",
            "\n",
            "Cross validation score obtained: 0.6668528193118356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "HG2qJqZw4m-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRCn099qi59m"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text clustering\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "Apply the listed clustering methods to the dataset:\n",
        "\n",
        "K-means\n",
        "\n",
        "DBSCAN\n",
        "\n",
        "Hierarchical clustering\n",
        "\n",
        "Word2Vec\n",
        "\n",
        "BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below. \n",
        "https://www.kaggle.com/karthik3890/text-clustering "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzHkzLEoi59n",
        "outputId": "adc70ecc-1961-47af-e2ba-59031e1a4908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "#Write your code here.\n",
        "#import libraries\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import Word\n",
        "nltk.download('wordnet')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_csv('Amazon_Unlocked_Mobile.csv')"
      ],
      "metadata": {
        "id": "z8B98qrd4ss2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape: {0}\".format(data_df.shape))"
      ],
      "metadata": {
        "id": "xIKv6X_k4veZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_data_df = data_df.head(1000)\n",
        "cluster_data_df"
      ],
      "metadata": {
        "id": "rY_5mB381HyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removal of special characters\n",
        "cluster_data_df['After noise removal'] = cluster_data_df['Reviews'].apply(lambda x: ''.join(re.sub(r\"[^a-zA-Z0-9]+\", ' ', charctr) for charctr in x ))\n",
        "\n",
        "#removal of punctations\n",
        "cluster_data_df['Punctuation removal'] = cluster_data_df['After noise removal'].str.replace('[^\\w\\s]','')\n",
        "\n",
        "# Removing numbers\n",
        "cluster_data_df['Remove numbers'] = cluster_data_df['Punctuation removal'].str.replace('\\d+', '')\n",
        "\n",
        "#removal of stopwords\n",
        "stop_word = stopwords.words('english')\n",
        "cluster_data_df['Stopwords removal'] = cluster_data_df['Remove numbers'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_word))\n",
        "\n",
        "# Lower Casing\n",
        "cluster_data_df['Lower casing'] = cluster_data_df['Stopwords removal'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "\n",
        "# Tokenization\n",
        "cluster_data_df['Tokenization'] = cluster_data_df['Lower casing'].apply(lambda x: TextBlob(x).words)\n",
        "\n",
        "# Stemming\n",
        "st = PorterStemmer()\n",
        "cluster_data_df['Stemming'] = cluster_data_df['Tokenization'].apply(lambda x: \" \".join([st.stem(word) for word in x]))\n",
        "\n",
        "# Lemmatization\n",
        "cluster_data_df['Lemmatization'] = cluster_data_df['Stemming'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "cluster_data_df"
      ],
      "metadata": {
        "id": "iOuMddLw41Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf = tfidf_vect.fit_transform(cluster_data_df['Lemmatization'].values)\n",
        "tfidf.shape"
      ],
      "metadata": {
        "id": "hYkwWtqj1SQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "model_tf = KMeans(n_clusters = 10, n_jobs = -1,random_state=99)\n",
        "model_tf.fit(tfidf)"
      ],
      "metadata": {
        "id": "c157tlXY1XOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_tf = model_tf.labels_\n",
        "cluster_center_tf=model_tf.cluster_centers_\n",
        "cluster_center_tf"
      ],
      "metadata": {
        "id": "EOZWfvyD1aDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terms1 = tfidf_vect.get_feature_names()"
      ],
      "metadata": {
        "id": "kVpGwiq81goR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terms1[1:10]"
      ],
      "metadata": {
        "id": "HaSaWzQ81hfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_score_tf = metrics.silhouette_score(tfidf, labels_tf, metric='euclidean')\n",
        "silhouette_score_tf"
      ],
      "metadata": {
        "id": "BYUJFQVi1lWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = cluster_data_df\n",
        "df1['Tfidf Clus Label'] = model_tf.labels_\n",
        "df1.head(5)"
      ],
      "metadata": {
        "id": "03Gy00Y71qyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.groupby(['Tfidf Clus Label'])['Reviews'].count()"
      ],
      "metadata": {
        "id": "IMCjEj3f1veI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top terms per cluster:\")\n",
        "order_centroids = model_tf.cluster_centers_.argsort()[:, ::-1]\n",
        "for i in range(10):\n",
        "    print(\"Cluster %d:\" % i, end='')\n",
        "    for ind in order_centroids[i, :10]:\n",
        "        print(' %s' % terms1[ind], end='')\n",
        "        print()"
      ],
      "metadata": {
        "id": "H7Y0utsJ1442"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar([x for x in range(10)], df1.groupby(['Tfidf Clus Label'])['Reviews'].count(), alpha = 0.4)\n",
        "plt.title('KMeans cluster points')\n",
        "plt.xlabel(\"Cluster number\")\n",
        "plt.ylabel(\"Number of points\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M8MXwVjr16Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(\"4 review of assigned to cluster \", i)\n",
        "    print(\"-\" * 70)\n",
        "    print(df1.iloc[df1.groupby(['Tfidf Clus Label']).groups[i][0]]['Reviews'])\n",
        "    print('\\n')\n",
        "    print(df1.iloc[df1.groupby(['Tfidf Clus Label']).groups[i][5]]['Reviews'])\n",
        "    print('\\n')\n",
        "    print(df1.iloc[df1.groupby(['Tfidf Clus Label']).groups[i][10]]['Reviews'])\n",
        "    print('\\n')\n",
        "    print(\"_\" * 70)"
      ],
      "metadata": {
        "id": "iQxeTvLB1_FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "bow = count_vect.fit_transform(cluster_data_df['Reviews'].values)\n",
        "bow.shape"
      ],
      "metadata": {
        "id": "SsW68N3G2E4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "model = KMeans(n_clusters = 10,init='k-means++', n_jobs = -1,random_state=99)\n",
        "model.fit(bow)"
      ],
      "metadata": {
        "id": "0mVv2Ns62L2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = model.labels_\n",
        "cluster_center=model.cluster_centers_"
      ],
      "metadata": {
        "id": "40W60Aj42O__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.silhouette_score(bow, labels, metric='euclidean'))"
      ],
      "metadata": {
        "id": "HeYqBqhO2SRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_data_df['Bow Clus Label'] = model.labels_ # the last column you can see the label numebers\n",
        "cluster_data_df.head(2)"
      ],
      "metadata": {
        "id": "k_6VYl5D2XTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "994XhlHC2dgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing 200th Nearest neighbour distance\n",
        "minPts = 2 * 100\n",
        "# Lower bound function copied from -> https://gist.github.com/m00nlight/0f9306b4d4e61ba0195f\n",
        "def lower_bound(nums, target): # This function return the number in the array just greater than or equal to itself.\n",
        "    l, r = 0, len(nums) - 1\n",
        "    while l <= r: # Binary searching.\n",
        "        mid = int(l + (r - l) / 2)\n",
        "        if nums[mid] >= target:\n",
        "            r = mid - 1\n",
        "        else:\n",
        "            l = mid + 1\n",
        "    return l\n",
        "\n",
        "def compute200thnearestneighbour(x, data): # Returns the distance of 200th nearest neighbour.\n",
        "    dists = []\n",
        "    for val in data:\n",
        "        dist = np.sum((x - val) **2 ) # computing distances.\n",
        "        if (len(dists) == 200 and dists[199] > dist): # If distance is larger than current largest distance found.\n",
        "          l = int(lower_bound(dists, dist)) # Using the lower bound function to get the right position.\n",
        "          if l < 200 and l >= 0 and dists[l] > dist:\n",
        "              dists[l] = dist\n",
        "        else:\n",
        "          dists.append(dist)\n",
        "          dists.sort()\n",
        "    \n",
        "    return dists[199] # Dist 199 contains the distance of 200th nearest neighbour."
      ],
      "metadata": {
        "id": "k7ZxWV592jxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_sent_train = list()\n",
        "\n",
        "for i in cluster_data_df[\"Lower casing\"].values:\n",
        "  list_of_sent_train.append(i.split())"
      ],
      "metadata": {
        "id": "hroffKHB2jkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "w2v_model=gensim.models.Word2Vec(list_of_sent_train, size=100, workers=4)"
      ],
      "metadata": {
        "id": "OQ7ddM_p2jLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this train\n",
        "count = 1\n",
        "for sent in list_of_sent_train: # for each review/sentence\n",
        "    sent_vec = np.zeros(100) # as word vectors are of zero length\n",
        "    cnt_words =1; # num of words with a valid vector in the sentence/review\n",
        "    for word in sent: # for each word in a review/sentence\n",
        "        try:\n",
        "            vec = w2v_model.wv[word]\n",
        "            sent_vec += vec\n",
        "            cnt_words += 1\n",
        "        except:\n",
        "            pass\n",
        "    sent_vec /= cnt_words\n",
        "    sent_vectors.append(sent_vec)\n",
        "sent_vectors = np.array(sent_vectors)\n",
        "sent_vectors = np.nan_to_num(sent_vectors)"
      ],
      "metadata": {
        "id": "ZrqDlM_f2ioC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twohundrethneigh = []\n",
        "for val in sent_vectors[:300]:\n",
        "    twohundrethneigh.append(compute200thnearestneighbour(val, sent_vectors[:300]) )\n",
        "twohundrethneigh.sort()"
      ],
      "metadata": {
        "id": "bhKnQLrp228L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DBSCAN(eps = 5, min_samples = minPts, n_jobs=-1)\n",
        "model.fit(sent_vectors)"
      ],
      "metadata": {
        "id": "7MDJtQj52if-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_data_df['AVG-W2V Clus Label'] = model.labels_\n",
        "cluster_data_df.head(2)"
      ],
      "metadata": {
        "id": "izcFqmbT2_L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "from scipy.cluster import hierarchy\n",
        "dendro = hierarchy.dendrogram(hierarchy.linkage(sent_vectors,method = 'ward'))\n",
        "plt.axhline(y = 10) # cut at 30 to get 5 clusters"
      ],
      "metadata": {
        "id": "UE1-6cxp2_CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')  #took n=5 from dendrogram curve \n",
        "Agg=cluster.fit_predict(sent_vectors)"
      ],
      "metadata": {
        "id": "StM6jKTP2-4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Giving Labels/assigning a cluster to each point/text \n",
        "aggdfa = cluster_data_df\n",
        "aggdfa['AVG-W2V Clus Label'] = cluster.labels_\n",
        "aggdfa.head(2)"
      ],
      "metadata": {
        "id": "JpM67Zm53POB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aggdfa.groupby(['AVG-W2V Clus Label'])['Reviews'].count()"
      ],
      "metadata": {
        "id": "zJ8OTui-3QRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(\"2 reviews of assigned to cluster \", i)\n",
        "    print(\"-\" * 70)\n",
        "    print(aggdfa.iloc[aggdfa.groupby(['AVG-W2V Clus Label']).groups[i][0]]['Reviews'])\n",
        "    print('\\n')\n",
        "    print(aggdfa.iloc[aggdfa.groupby(['AVG-W2V Clus Label']).groups[i][1]]['Reviews'])\n",
        "    print('\\n')\n",
        "    print(\"_\" * 70)"
      ],
      "metadata": {
        "id": "93h551xi3QJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kfeXM7Ta3Z_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI00bWP_i59n"
      },
      "source": [
        "In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bW8BuPni59n"
      },
      "outputs": [],
      "source": [
        "#You can write you answer here. (No code needed)\n",
        "\n",
        "k-means splis a large dataset into a fixed number of clustes and generalises them into different sizes and shape. It doesnt work much efficiently with noisy data but it works efficiently with large datasets. \n",
        "DBscan is used to sepeate clusters that has high density from the clusters with low density. It works efficiently for noisy data but does not work efficiently with large datasets. \n",
        "Hierarchial Clustering is used to group objects that are similar into groups that are called as clusters. It is challenging to classify the clusters using Hierarchical c\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}